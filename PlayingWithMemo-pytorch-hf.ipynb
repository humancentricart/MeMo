{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906bcd7e-a96f-4a12-a157-634555d41a0a",
   "metadata": {},
   "source": [
    "# MeMo HF\n",
    "Version integrated with Transformer Libraries (Version 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb8646-500b-4d4e-872b-0f2bc51f8a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:36:56.332071Z",
     "start_time": "2025-01-16T17:36:33.342424Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from MeMoHF.modelling_memo_tokenizer import MeMoTokenizer\n",
    "from MeMoHF.modelling_memo_configuration import MeMoConfig\n",
    "from MeMoHF.modelling_memo import MeMoForCausalLM\n",
    "from MeMoHF.evaluating_memo import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981a54a2-6b37-4ea1-82b4-1f76b65d90ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'MeMoTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token and pad token id = <|endoftext|>, 0\n"
     ]
    }
   ],
   "source": [
    "max_length = 12 \n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          truncation_side = 'left',\n",
    "                                          padding_side='left', max_length=max_length, head_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3536ed-17c1-4508-bf6f-811069639bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[18886,   256, 36144,  4164,  1809,    80,  1448,   295,   532,  1584,\n",
      "            13, 50190]]), tensor([[  256, 36144,  4164,  1809,    80,  1448,   295,   532,  1584,    13,\n",
      "         50190,    15]]))\n"
     ]
    }
   ],
   "source": [
    "with open(\"testo_di_prova.txt\") as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()\n",
    "\n",
    "token_ids = tokenizer.encode(my_first_text)#, return_tensors='pt')\n",
    "print(token_ids) # return max len + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6ed262-7cee-4729-acf4-3e79bd7797b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']), torch.Size([52, 12]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text, my_first_text[0:10]])\n",
    "memo_input.keys(), memo_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244820ac-b02f-4e26-9c24-78fb5512d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Cosimo di\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Cosimo di Giovanni\n",
      "\n",
      " de' Medici detto il Vecchio o Pater\n",
      "' Medici detto il Vecchio o Pater patri\n",
      "\n",
      "æ (Firenze, 27 settembre 1389\n",
      " (Firenze, 27 settembre 1389 –\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokenizer.decode(memo_input['input_ids'][i]))\n",
    "    print(tokenizer.decode(memo_input['labels'][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176aa12-77ef-4757-91d6-f7014a5efe69",
   "metadata": {},
   "source": [
    "Check memorization on single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aedcfe5-ed74-42da-a4e8-7c8939a44605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text, my_first_text[10:30]])\n",
    "\n",
    "memo_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab6c672-5009-4e77-9560-9cb354c418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoHF.modelling_memo_embedding import MeMoEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6307541-5a34-4402-a658-36cae4fd7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "d,h,l = 1024, 4, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8fd0b85-044a-4381-af5a-ffe70bd9ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n"
     ]
    }
   ],
   "source": [
    "embedding = MeMoEmbedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim=d,\n",
    "    padding_idx=tokenizer.pad_token_id, #0\n",
    "    _freeze=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce746c9-9459-43d3-93b9-2873999a69f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 12, 1024]), torch.Size([52, 12, 1024]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = embedding.encode(memo_input['input_ids'])\n",
    "output_symbols = embedding.encode(memo_input['labels'])\n",
    "\n",
    "input_embeddings.shape, output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3affa221-c507-44fc-b069-a3e727b95978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         5089],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 2447, 6945,  287,\n",
      "         6004]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0130,  0.0025, -0.0328,  ..., -0.0017, -0.0307,  0.0098]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.0214, -0.0625,  0.0228,  ...,  0.0210,  0.0681, -0.0045],\n",
       "         [-0.0002, -0.0390,  0.0355,  ...,  0.0277, -0.0276,  0.0295],\n",
       "         [ 0.0354, -0.0010,  0.0389,  ..., -0.0259, -0.0141, -0.0456]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_ids = tokenizer(['Test', 'Un altro Test'])['input_ids']\n",
    "print(input_tokens_ids)\n",
    "\n",
    "input_embeddings = embedding.forward(input_tokens_ids)\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f32b2ea-3be1-49bd-9c40-2962adb4d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoHF.modelling_memo_layer import MeMoLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce1f7aa-cdc1-4f17-9c8b-12a48eb7c56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeMoLayer(\n",
       "  (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "  (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "  (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MeMoLayer(d, h)\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93091d-2818-49d7-9170-8eea03ddfce5",
   "metadata": {},
   "source": [
    "Memo: Initializing the Tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55d6ff0-4a27-44cc-9593-1bdf8f53f721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:38:05.140165Z",
     "start_time": "2025-01-16T17:38:04.141063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'MeMoTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token and pad token id = <|endoftext|>, 0\n",
      "MeMo embedding initilialization\n",
      "GPU: NVIDIA RTX A6000 is available.\n"
     ]
    }
   ],
   "source": [
    "# Meta Parameters : \n",
    "#    d - inner dimension\n",
    "#    h - number of heads\n",
    "#    l - number of layers\n",
    "d,h,l = 1024, 4, 4\n",
    "chunk_length = 1024\n",
    "\n",
    "# Initializing a standard Tokenizer\n",
    "max_length = chunk_length \n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          padding_side='left', truncation_side='left', \n",
    "                                          max_length=max_length, head_number=h)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Intializing Memo Configuration\n",
    "config = MeMoConfig(vocab_size=tokenizer.vocab_size, \n",
    "               hidden_size=d, \n",
    "               num_hidden_layers=l,\n",
    "               num_attention_heads=h,\n",
    "               chunk_length=chunk_length,\n",
    "               bos_token_id=tokenizer.bos_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "              )\n",
    "\n",
    "# Initializing the Memo Model from the configuration\n",
    "\n",
    "model = MeMoForCausalLM(config) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "    model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d8383-7bfe-45c6-925e-f3fce09e6abc",
   "metadata": {},
   "source": [
    "Reading the two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56448131-9ce4-4e35-ae1c-a1190a1bbb79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:38:09.762044Z",
     "start_time": "2025-01-16T17:38:09.730794Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"testo_di_prova.txt\") as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()\n",
    "with open(\"testo_di_prova2.txt\") as my_first_text_f:\n",
    "    my_second_text = my_first_text_f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c94e47-0d33-4478-835a-1fe5ead05303",
   "metadata": {},
   "source": [
    "Memorizing the first text and evaluating if it is memorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d76c36-d35c-4d47-b3f4-0fe172901dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:38:20.913676Z",
     "start_time": "2025-01-16T17:38:12.289894Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [00:01<00:00, 588.20it/s]\n",
      "100%|██████████| 1015/1015 [00:02<00:00, 448.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.7025)\n",
      "Memorization level of second text :  tensor(0.0224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "memo_input_1 = tokenizer.get_text_batch_encoding([my_first_text]*8)  # Writing the same doc 8 times to stress the memorization with batch\n",
    "memo_input_2 = tokenizer.get_text_batch_encoding([my_second_text]*8) # Writing the same doc 8 times to stress the memorization with batch\n",
    "\n",
    "model.memorize_text(memo_input_1)\n",
    "e = Evaluation()\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75c848-e9a8-402b-9709-798ad40b5d87",
   "metadata": {},
   "source": [
    "Memorizing the second text and checking if it affected the memorization of the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234196b2-0f00-4f08-90a7-48592623e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [00:01<00:00, 597.13it/s]\n",
      "100%|██████████| 1015/1015 [00:02<00:00, 432.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.3589)\n",
      "Memorization level of second text :  tensor(0.2348)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.memorize_text(memo_input_2)\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07882124-35fe-48fc-b359-c260f06f51a7",
   "metadata": {},
   "source": [
    "Forgetting the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1beed2e-e108-4a31-bcdb-1d821af65c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forget_text(memo_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3dcb-a0c2-41fe-9b4d-7b8ff7726910",
   "metadata": {},
   "source": [
    "Checking the effect on the two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b86df2dd-81e9-4997-9fe1-ab715da04641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [00:02<00:00, 499.64it/s]\n",
      "100%|██████████| 1015/1015 [00:02<00:00, 433.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.6979)\n",
      "Memorization level of second text :  tensor(0.0208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd353399-4c63-468c-8b02-371d12390b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeMoForCausalLM(\n",
       "  (memo): MeMo(\n",
       "    (encoder): MeMoEmbedding(50254, 1024, padding_idx=0)\n",
       "    (layers): MeMoLayers(\n",
       "      (0-3): 4 x MeMoLayer(\n",
       "        (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "        (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "        (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): MeMoEmbedding(50254, 1024, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8384ba95-0d56-4171-9312-e715148528f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n",
      "CMM pre learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memorizing the same text iteration = 0\n",
      "memorizing the same text iteration = 1\n",
      "memorizing the same text iteration = 2\n",
      "memorizing the same text iteration = 3\n",
      "memorizing the same text iteration = 4\n",
      "memorizing the same text iteration = 5\n",
      "memorizing the same text iteration = 6\n",
      "memorizing the same text iteration = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.8685e-01,  2.3611e-03,  5.7158e-03,  ..., -2.0955e-02,\n",
       "         -7.8387e-03, -3.0100e-02],\n",
       "        [ 2.3611e-03,  1.0030e+00, -4.8353e-03,  ..., -6.8917e-03,\n",
       "          3.6630e-03,  4.4900e-03],\n",
       "        [ 5.7158e-03, -4.8353e-03,  1.0183e+00,  ...,  1.7557e-02,\n",
       "         -5.1791e-05,  1.4415e-02],\n",
       "        ...,\n",
       "        [-2.0955e-02, -6.8917e-03,  1.7557e-02,  ...,  1.0093e+00,\n",
       "          3.6133e-03, -1.3998e-02],\n",
       "        [-7.8387e-03,  3.6630e-03, -5.1791e-05,  ...,  3.6133e-03,\n",
       "          9.9020e-01, -2.5500e-02],\n",
       "        [-3.0100e-02,  4.4900e-03,  1.4415e-02,  ..., -1.3998e-02,\n",
       "         -2.5500e-02,  1.0135e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0238,  0.0085, -0.0166,  ...,  0.0067,  0.0305, -0.0151],\n",
       "        [-0.0216,  0.0119, -0.0180,  ...,  0.0093,  0.0123,  0.0514],\n",
       "        [-0.0219,  0.0268,  0.0105,  ..., -0.0112,  0.0089, -0.0074],\n",
       "        ...,\n",
       "        [ 0.0114, -0.0462,  0.0281,  ...,  0.0463, -0.0177, -0.0195],\n",
       "        [-0.0349,  0.0236,  0.0352,  ...,  0.0020,  0.0098,  0.0043],\n",
       "        [ 0.0151, -0.0149, -0.0279,  ..., -0.0244,  0.0027, -0.0331]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 767/767 [00:06<00:00, 122.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization after memorizing: %f  tensor(0.9156)\n",
      "forgetting the same text iteration = 0\n",
      "forgetting the same text iteration = 1\n",
      "forgetting the same text iteration = 2\n",
      "forgetting the same text iteration = 3\n",
      "forgetting the same text iteration = 4\n",
      "forgetting the same text iteration = 5\n",
      "forgetting the same text iteration = 6\n",
      "forgetting the same text iteration = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 767/767 [00:05<00:00, 144.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization after forgetting: %f  tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = MeMoConfig(vocab_size=tokenizer.vocab_size, \n",
    "               hidden_size=d, \n",
    "               num_hidden_layers=l,\n",
    "               num_attention_heads=h,\n",
    "               chunk_length=chunk_length,\n",
    "               bos_token_id=tokenizer.bos_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "              )\n",
    "\n",
    "# Initializing the Memo Model from the configuration\n",
    "\n",
    "model = MeMoForCausalLM(config)\n",
    "print(\"CMM pre learning\")\n",
    "display(model.memo.layers[0].CMM.weight)\n",
    "\n",
    "\n",
    "bs = 8\n",
    "for b in range(bs):\n",
    "    print(f\"memorizing the same text iteration = {b}\")\n",
    "    memo_input = tokenizer.get_text_batch_encoding(my_first_text)\n",
    "    model.memorize_text(memo_input)\n",
    "\n",
    "Prj = model.memo.layers[0].Prj.weight.detach().cpu()\n",
    "CMM = model.memo.layers[0].CMM.weight.detach().cpu()\n",
    "\n",
    "display(Prj.T @ Prj)\n",
    "display(CMM)\n",
    "\n",
    "e = Evaluation()\n",
    "out = e.check_pretokenized(model, tokenizer, memo_input['input_ids'])\n",
    "print(\"Degree of memorization after memorizing: %f \", out)\n",
    "\n",
    "for b in range(bs):\n",
    "    print(f\"forgetting the same text iteration = {b}\")\n",
    "    memo_input = tokenizer.get_text_batch_encoding(my_first_text)\n",
    "    model.forget_text(memo_input)\n",
    "\n",
    "out = e.check_pretokenized(model, tokenizer, memo_input['input_ids'])\n",
    "print(\"Degree of memorization after forgetting: %f \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeab644-7309-4880-84ec-f8a417c8e5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
