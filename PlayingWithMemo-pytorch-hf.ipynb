{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906bcd7e-a96f-4a12-a157-634555d41a0a",
   "metadata": {},
   "source": [
    "# MeMo HF\n",
    "Version integrated with Transformer Libraries (Version 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1fb8646-500b-4d4e-872b-0f2bc51f8a2e",
   "metadata": {},
   "source": [
    "import torch\n",
    "from MeMoHF.modelling_memo_tokenizer import MeMoTokenizer\n",
    "from MeMoHF.modelling_memo_configuration import MeMoConfig\n",
    "from MeMoHF.modelling_memo import MeMoForCausalLM\n",
    "from MeMoHF.evaluating_memo import Evaluation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf93091d-2818-49d7-9170-8eea03ddfce5",
   "metadata": {},
   "source": [
    "Memo: Initializing the Tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "f55d6ff0-4a27-44cc-9593-1bdf8f53f721",
   "metadata": {},
   "source": [
    "# Meta Parameters : \n",
    "#    d - inner dimension\n",
    "#    h - number of heads\n",
    "#    l - number of layers\n",
    "d,h,l = 2048, 8, 3\n",
    "chunk_length = 4096\n",
    "\n",
    "# Initializing a standard Tokenizer\n",
    "max_length = chunk_length \n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          padding_side='left', truncation_side='left', \n",
    "                                          max_length=max_length, head_number=h)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Intializing Memo Configuration\n",
    "config = MeMoConfig(vocab_size=tokenizer.vocab_size, \n",
    "               hidden_size=d, \n",
    "               num_hidden_layers=l,\n",
    "               num_attention_heads=h,\n",
    "               chunk_length=chunk_length,\n",
    "               bos_token_id=tokenizer.bos_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "              )\n",
    "\n",
    "# Initializing the Memo Model from the configuration\n",
    "\n",
    "model = MeMoForCausalLM(config) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "    model.to('cuda')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b1d8383-7bfe-45c6-925e-f3fce09e6abc",
   "metadata": {},
   "source": [
    "Reading the two texts"
   ]
  },
  {
   "cell_type": "code",
   "id": "56448131-9ce4-4e35-ae1c-a1190a1bbb79",
   "metadata": {},
   "source": [
    "with open(\"testo_di_prova.txt\") as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()\n",
    "with open(\"testo_di_prova2.txt\") as my_first_text_f:\n",
    "    my_second_text = my_first_text_f.read()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77c94e47-0d33-4478-835a-1fe5ead05303",
   "metadata": {},
   "source": [
    "Memorizing the first text and evaluating if it is memorized"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4d76c36-d35c-4d47-b3f4-0fe172901dd3",
   "metadata": {},
   "source": [
    "memo_input_1 = tokenizer.get_text_batch_encoding([my_first_text]*8)  # Writing the same doc 8 times to stress the memorization with batch\n",
    "memo_input_2 = tokenizer.get_text_batch_encoding([my_second_text]*8) # Writing the same doc 8 times to stress the memorization with batch\n",
    "\n",
    "model.memorize_text(memo_input_1)\n",
    "e = Evaluation()\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1[0]['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2[0]['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec75c848-e9a8-402b-9709-798ad40b5d87",
   "metadata": {},
   "source": [
    "Memorizing the second text and checking if it affected the memorization of the first text"
   ]
  },
  {
   "cell_type": "code",
   "id": "234196b2-0f00-4f08-90a7-48592623e653",
   "metadata": {},
   "source": [
    "model.memorize_text(memo_input_2)\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1[0]['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2[0]['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "07882124-35fe-48fc-b359-c260f06f51a7",
   "metadata": {},
   "source": [
    "Forgetting the first document"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1beed2e-e108-4a31-bcdb-1d821af65c6b",
   "metadata": {},
   "source": [
    "model.forget_text(memo_input_2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dcef3dcb-a0c2-41fe-9b4d-7b8ff7726910",
   "metadata": {},
   "source": [
    "Checking the effect on the two texts"
   ]
  },
  {
   "cell_type": "code",
   "id": "b86df2dd-81e9-4997-9fe1-ab715da04641",
   "metadata": {},
   "source": [
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1[0]['input_ids'], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2[0]['input_ids'], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8384ba95-0d56-4171-9312-e715148528f0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
