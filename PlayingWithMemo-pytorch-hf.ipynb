{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906bcd7e-a96f-4a12-a157-634555d41a0a",
   "metadata": {},
   "source": [
    "# MeMo HF\n",
    "Version integrated with Transformer Libraries (Version 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb8646-500b-4d4e-872b-0f2bc51f8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from MeMoHF.modelling_memo_tokenizer import MeMoTokenizer\n",
    "from MeMoHF.modelling_memo_configuration import MeMoConfig\n",
    "from MeMoHF.modelling_memo import MeMoForCausalLM\n",
    "from MeMoHF.evaluating_memo import Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93091d-2818-49d7-9170-8eea03ddfce5",
   "metadata": {},
   "source": [
    "Memo: Initializing the Tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55d6ff0-4a27-44cc-9593-1bdf8f53f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'MeMoTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token and pad token id = <|endoftext|>, 0\n",
      "MeMo embedding initilialization\n",
      "GPU: NVIDIA RTX A6000 is available.\n"
     ]
    }
   ],
   "source": [
    "# Meta Parameters : \n",
    "#    d - inner dimension\n",
    "#    h - number of heads\n",
    "#    l - number of layers\n",
    "d,h,l = 2048, 8, 3\n",
    "chunk_length = 4096\n",
    "\n",
    "# Initializing a standard Tokenizer\n",
    "max_length = chunk_length \n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          padding_side='left', truncation_side='left', \n",
    "                                          max_length=max_length, head_number=h)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Intializing Memo Configuration\n",
    "config = MeMoConfig(vocab_size=tokenizer.vocab_size, \n",
    "               hidden_size=d, \n",
    "               num_hidden_layers=l,\n",
    "               num_attention_heads=h,\n",
    "               chunk_length=chunk_length,\n",
    "               bos_token_id=tokenizer.bos_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "              )\n",
    "\n",
    "# Initializing the Memo Model from the configuration\n",
    "\n",
    "model = MeMoForCausalLM(config) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "    model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d8383-7bfe-45c6-925e-f3fce09e6abc",
   "metadata": {},
   "source": [
    "Reading the two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56448131-9ce4-4e35-ae1c-a1190a1bbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testo_di_prova.txt\",encoding='utf8') as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()\n",
    "with open(\"testo_di_prova2.txt\",encoding='utf8') as my_first_text_f:\n",
    "    my_second_text = my_first_text_f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c94e47-0d33-4478-835a-1fe5ead05303",
   "metadata": {},
   "source": [
    "Memorizing the first text and evaluating if it is memorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d76c36-d35c-4d47-b3f4-0fe172901dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4087/4087 [00:22<00:00, 184.60it/s]\n",
      "100%|██████████| 4087/4087 [00:22<00:00, 183.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.9907)\n",
      "Memorization level of second text :  tensor(0.0334)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "memo_input_1 = tokenizer.get_text_batch_encoding([my_first_text]*1)  # Writing the same doc 8 times to stress the memorization with batch\n",
    "memo_input_2 = tokenizer.get_text_batch_encoding([my_second_text]*1) # Writing the same doc 8 times to stress the memorization with batch\n",
    "\n",
    "model.memorize_text(memo_input_1)\n",
    "e = Evaluation()\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'][:,0,:], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'][:,0,:], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75c848-e9a8-402b-9709-798ad40b5d87",
   "metadata": {},
   "source": [
    "Memorizing the second text and checking if it affected the memorization of the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234196b2-0f00-4f08-90a7-48592623e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4087/4087 [00:22<00:00, 181.28it/s]\n",
      "100%|██████████| 4087/4087 [00:22<00:00, 179.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.9722)\n",
      "Memorization level of second text :  tensor(0.9785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.memorize_text(memo_input_2)\n",
    "\n",
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'][:,0,:], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'][:,0,:], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07882124-35fe-48fc-b359-c260f06f51a7",
   "metadata": {},
   "source": [
    "Forgetting the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1beed2e-e108-4a31-bcdb-1d821af65c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forget_text(memo_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3dcb-a0c2-41fe-9b4d-7b8ff7726910",
   "metadata": {},
   "source": [
    "Checking the effect on the two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86df2dd-81e9-4997-9fe1-ab715da04641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4087/4087 [00:22<00:00, 179.19it/s]\n",
      "100%|██████████| 4087/4087 [00:22<00:00, 178.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorization level of first text  :  tensor(0.9907)\n",
      "Memorization level of second text :  tensor(0.0350)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "e1 = e.check_pretokenized(model, tokenizer, memo_input_1['input_ids'][:,0,:], starting_point=8)\n",
    "e2 = e.check_pretokenized(model, tokenizer, memo_input_2['input_ids'][:,0,:], starting_point=8)\n",
    "\n",
    "print(\"Memorization level of first text  : \", e1) \n",
    "print(\"Memorization level of second text : \", e2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8384ba95-0d56-4171-9312-e715148528f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input_1['input_ids'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
