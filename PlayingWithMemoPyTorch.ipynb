{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd95042e-0d56-401d-ae0c-0fbe1eee44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A6000 is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa20327-eefe-4d30-910d-f02605c09a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  6 14:53:33 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              27W / 300W |     25MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              21W / 300W |   4682MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac14a52-6180-4c0d-997e-b24745c624f9",
   "metadata": {},
   "source": [
    "## MeMo Tokenizer and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7956914c-ec7b-4fa7-a6a2-dbff28f8cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoPyTorch.modelling_memo_tokenizer import MeMoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8673f471-46ce-4875-b74d-e8a0378a01ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'MeMoTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token and pad token id = <|endoftext|>, 0\n"
     ]
    }
   ],
   "source": [
    "max_length = 12 \n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          truncation_side = 'left',\n",
    "                                          padding_side='left', max_length=max_length, head_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c159d5f5-2bf5-4d04-bde2-c897be8f494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[18886,   256, 36144,  4164,  1809,    80,  1448,   295,   532,  1584,\n",
      "            13, 50190]]), tensor([[  256, 36144,  4164,  1809,    80,  1448,   295,   532,  1584,    13,\n",
      "         50190,    15]]))\n"
     ]
    }
   ],
   "source": [
    "with open(\"testo_di_prova.txt\") as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()\n",
    "\n",
    "token_ids = tokenizer.encode(my_first_text)#, return_tensors='pt')\n",
    "print(token_ids) # return max len + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c458fb-4eeb-4ffd-91d6-0f22d4d6566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text, my_first_text[0:10]])\n",
    "memo_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427781b3-ae1c-430a-b961-6500d44c4100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb7a04-5b90-48f5-be13-96ca53ce73a7",
   "metadata": {},
   "source": [
    "## MeMo Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e6eeb9-4f10-4fb6-b6c4-53dcfedca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoPyTorch.modelling_memo_embedding import MeMoEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94661fdd-cef9-4a41-b197-9d5f40645e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "d,h,l = 1024, 4, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2612d880-6777-4052-9b05-f32fb1f6dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n"
     ]
    }
   ],
   "source": [
    "embedding = MeMoEmbedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim=d,\n",
    "    padding_idx=tokenizer.pad_token_id, #0\n",
    "    _freeze=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3835b9f2-c1a7-4673-b9a4-ba5710fc0df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         5089],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 2447, 6945,  287,\n",
      "         6004]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0835, -0.0631, -0.0027,  ..., -0.0271, -0.0230,  0.0581]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.0217,  0.0062, -0.0081,  ..., -0.0301, -0.0055, -0.0123],\n",
       "         [ 0.0048,  0.0056, -0.0090,  ...,  0.0085, -0.0227, -0.0020],\n",
       "         [-0.0105,  0.0004, -0.0299,  ...,  0.0073,  0.0339,  0.0490]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_ids = tokenizer(['Test', 'Un altro Test'])['input_ids']\n",
    "print(input_tokens_ids)\n",
    "\n",
    "input_embeddings = embedding.forward(input_tokens_ids)\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf18be91-e462-43c4-adb8-fceb129469e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text, my_first_text[10:30]])\n",
    "\n",
    "memo_input[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba11bf1-12c3-4b6e-96e0-7009ffb0d60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0, 38577,\n",
       "         17622,  1073],\n",
       "        [  372,     8,  9718,    74,   843,   936,  4164, 43876, 41380,   258,\n",
       "           367,   727],\n",
       "        [ 5507,   313, 15723,   445,  2721,    13,  3435,  3414,   358,  3381,\n",
       "         15410,    26],\n",
       "        [ 9776,  1266,    74,    13,   337, 11703,   639, 39337,  1638,  1540,\n",
       "            10, 12187],\n",
       "        [  440,  2314,  4173,   299,  8913,  2942,   250,   352,  6770,    80,\n",
       "            13,  2248],\n",
       "        [  861,   410,   372, 32924,  1073, 33813,   445,  2721,   299,  2248,\n",
       "            80,  1484],\n",
       "        [ 1073,   659,  4611,  1073,   391,   300,   466,  5711, 14804,  1431,\n",
       "           304, 19702],\n",
       "        [   74,    15, 14929,  1327,  1323, 10081, 24843, 15438,   412, 16406,\n",
       "         38055,  9821],\n",
       "        [ 3737,  1073,   391,   300,   466,  5711, 39814,   260,   770,  5991,\n",
       "           313,  1962],\n",
       "        [18006, 22217, 42722, 10863,   262,  7958,  1593, 12704,  5940,  2719,\n",
       "           538, 21287]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input[0]['input_ids'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a26e8e-22f3-499a-bba2-f3f714debdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 12, 1024]), torch.Size([52, 12, 1024]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = embedding.encode(memo_input[0]['input_ids'])\n",
    "output_symbols = embedding.encode(memo_input[0]['labels'])\n",
    "\n",
    "input_embeddings.shape, output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3adbd482-ce71-4d86-a049-a9ef8737ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0, 38577,\n",
       "         17622,  1073],\n",
       "        [  372,     8,  9718,    74,   843,   936,  4164, 43876, 41380,   258,\n",
       "           367,   727],\n",
       "        [ 5507,   313, 15723,   445,  2721,    13,  3435,  3414,   358,  3381,\n",
       "         15410,    26],\n",
       "        [ 9776,  1266,    74,    13,   337, 11703,   639, 39337,  1638,  1540,\n",
       "            10, 12187],\n",
       "        [  440,  2314,  4173,   299,  8913,  2942,   250,   352,  6770,    80,\n",
       "            13,  2248],\n",
       "        [  861,   410,   372, 32924,  1073, 33813,   445,  2721,   299,  2248,\n",
       "            80,  1484],\n",
       "        [ 1073,   659,  4611,  1073,   391,   300,   466,  5711, 14804,  1431,\n",
       "           304, 19702],\n",
       "        [   74,    15, 14929,  1327,  1323, 10081, 24843, 15438,   412, 16406,\n",
       "         38055,  9821],\n",
       "        [ 3737,  1073,   391,   300,   466,  5711, 39814,   260,   770,  5991,\n",
       "           313,  1962],\n",
       "        [18006, 22217, 42722, 10863,   262,  7958,  1593, 12704,  5940,  2719,\n",
       "           538, 21287]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded, _ = embedding.decode(input_embeddings)\n",
    "print(decoded.shape)\n",
    "\n",
    "decoded[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616e69f2-501c-41fc-8f7e-21b6b3ae2f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded[0:10] == memo_input[0]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776eb6ee-c5ec-4b24-afb2-127832d4fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  9.7935e-01,  1.2794e-02,  ...,  6.0318e-02,\n",
       "          2.1276e-02,  2.9399e-02],\n",
       "        [ 0.0000e+00,  1.2794e-02,  9.6680e-01,  ...,  3.8886e-02,\n",
       "         -1.7371e-02,  5.5490e-02],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  6.0318e-02,  3.8886e-02,  ...,  9.6188e-01,\n",
       "          6.3878e-02, -6.1192e-04],\n",
       "        [ 0.0000e+00,  2.1276e-02, -1.7371e-02,  ...,  6.3878e-02,\n",
       "          9.8433e-01, -4.7447e-02],\n",
       "        [ 0.0000e+00,  2.9399e-02,  5.5490e-02,  ..., -6.1192e-04,\n",
       "         -4.7447e-02,  9.7612e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50250.5938)\n",
      "tensor(-1252.6641)\n"
     ]
    }
   ],
   "source": [
    "sims = embedding.weight @ embedding.weight.T\n",
    "display(sims)\n",
    "diag_sum = torch.sum(sims[1:, 1: ].diag()) # almost 1 in each entry\n",
    "print(diag_sum) # obs vs expected\n",
    "print(torch.sum(sims[1:, 1:]) - diag_sum) #almost 0... more or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68109c97-0d07-4737-a2e1-1506927e7897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d65c6b2f-7994-4035-bf57-5e389e6c0db7",
   "metadata": {},
   "source": [
    "## Test layer and MeMo CMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851420e5-b642-4196-9d39-81ec08bbffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoPyTorch.modelling_memo_layer import MeMoLayer, ProjectionSequence, ProjectionTokens, CorrelationMatrixMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca806ba1-8230-4e22-8171-a96695a0059c",
   "metadata": {},
   "source": [
    "### Check initialization of each matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6ef81f9-02b6-4236-9542-7ec40f80a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1024]) (trasposed wrt saved one) in_features=4096, out_features=1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0144, 1.0094, 1.0184,  ..., 0.9896, 0.9906, 0.9925],\n",
       "        grad_fn=<DiagonalBackward0_copy>),\n",
       " tensor([0.2668, 0.2575, 0.2495,  ..., 0.2568, 0.2474, 0.2405],\n",
       "        grad_fn=<DiagonalBackward0_copy>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,h,l = 1024, 4, 3\n",
    "proj = ProjectionSequence(d, d*h)\n",
    "print(proj.weight.shape, proj.extra_repr())\n",
    "(proj.weight.T @ proj.weight).diag(), (proj.weight @ proj.weight.T).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b310ff39-9e6c-420e-9062-d83abfa8ef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0223, 0.9927, 1.0043,  ..., 1.0023, 1.0144, 1.0078]),\n",
       " tensor([0.2474, 0.2642, 0.2357,  ..., 0.2390, 0.2545, 0.2311]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prj = torch.normal(0, 1/math.sqrt(d*h), size=(d,d*h))\n",
    "Prj = torch.transpose(Prj, 0, 1)\n",
    "(Prj.T @ Prj).diag(), (Prj @ Prj.T).diag()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfe9a656-6558-47a2-a49a-1fcf73865c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f1066d-be0d-4772-b06a-4b2cc02fe98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "4\n",
      "torch.Size([256, 1024]) in_features=1024, out_features=256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0034, 1.1309, 1.0402,  ..., 0.9823, 0.8685, 1.1207],\n",
       "        grad_fn=<DiagonalBackward0_copy>),\n",
       " tensor([3.7838, 3.8727, 3.8892, 4.1692, 3.7304, 4.0257, 3.6414, 4.1350, 3.9648,\n",
       "         3.9609, 3.6953, 3.9883, 4.0704, 3.9424, 4.0839, 3.9512, 4.2426, 3.9462,\n",
       "         3.8508, 4.1929, 3.8313, 4.0518, 4.0457, 4.1839, 4.0315, 4.2982, 3.7961,\n",
       "         4.0107, 4.0511, 4.3714, 3.7917, 4.0835, 3.8269, 3.9315, 4.2685, 3.9030,\n",
       "         4.0416, 3.9057, 3.9971, 3.9861, 4.2417, 3.8572, 4.0085, 3.8302, 3.7265,\n",
       "         4.2387, 3.8386, 3.8492, 3.9907, 4.0602, 4.0078, 3.7434, 4.0288, 4.1997,\n",
       "         4.1648, 3.9174, 4.1524, 3.6851, 4.0384, 3.9745, 3.9933, 3.9400, 4.2912,\n",
       "         3.9150, 3.8510, 4.3452, 4.2322, 3.9411, 4.1291, 4.3399, 4.3190, 3.7528,\n",
       "         4.0245, 3.9443, 3.6496, 4.0652, 3.7083, 4.1614, 3.7873, 3.9109, 4.1132,\n",
       "         3.5621, 4.1639, 4.1976, 3.6749, 4.1739, 4.1141, 3.7559, 4.3099, 4.0219,\n",
       "         4.2101, 4.0856, 3.9498, 4.1303, 3.9624, 4.0415, 3.9125, 4.4438, 3.7975,\n",
       "         4.2252, 4.0450, 4.0151, 4.1600, 3.8376, 3.8800, 4.0557, 3.9770, 3.8892,\n",
       "         4.2558, 4.2264, 4.2783, 3.9937, 3.7926, 3.9557, 3.8881, 4.3422, 4.0910,\n",
       "         4.1245, 4.0119, 4.2801, 4.0548, 4.0592, 3.6693, 3.6636, 4.0123, 3.8223,\n",
       "         4.0342, 4.0025, 3.8216, 4.0721, 4.1656, 4.2985, 4.2463, 3.6781, 4.0603,\n",
       "         3.7865, 4.2509, 3.9922, 3.9155, 4.2942, 4.0879, 3.8928, 4.1098, 3.7673,\n",
       "         4.1311, 4.3831, 3.9680, 3.6472, 3.7721, 3.9871, 3.8708, 4.2869, 3.7408,\n",
       "         4.1303, 4.1221, 3.8699, 3.9455, 3.9901, 3.7916, 3.7795, 3.9862, 4.0593,\n",
       "         3.9467, 4.1750, 4.1790, 4.0428, 4.1825, 4.3333, 4.1065, 4.4822, 3.9706,\n",
       "         4.0974, 4.4451, 3.8862, 3.8632, 3.9182, 3.7779, 4.1039, 3.7929, 3.7045,\n",
       "         4.0780, 4.1313, 3.8923, 3.8969, 3.8503, 3.8015, 4.2050, 4.0515, 4.2852,\n",
       "         3.8931, 3.8274, 3.9473, 4.0068, 3.8398, 3.9253, 4.0463, 4.2653, 4.5713,\n",
       "         4.1978, 4.0603, 4.1490, 3.8440, 3.9615, 3.9875, 4.0122, 3.8948, 3.9722,\n",
       "         3.8703, 3.8708, 4.2306, 4.0369, 3.9662, 3.7374, 3.6107, 3.8696, 4.0832,\n",
       "         3.9758, 4.2820, 3.8688, 4.1640, 4.0576, 4.1040, 3.8529, 4.1677, 4.0840,\n",
       "         4.2973, 3.8945, 3.6775, 4.3037, 4.1577, 3.7388, 3.8299, 4.1175, 4.0280,\n",
       "         3.9732, 3.8597, 3.9626, 3.5869, 4.1025, 4.0700, 4.2813, 3.9624, 3.9332,\n",
       "         4.2128, 3.7406, 4.1143, 4.0097, 3.9001, 3.9449, 4.1837, 3.9720, 3.9663,\n",
       "         4.0007, 4.2209, 3.6811, 3.7977], grad_fn=<DiagonalBackward0_copy>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d)\n",
    "print(h)\n",
    "\n",
    "d_k = d // h\n",
    "\n",
    "W_v = ProjectionTokens(d, d_k)\n",
    "\n",
    "print(W_v.weight.shape, W_v.extra_repr())\n",
    "\n",
    "\n",
    "### always used transposed! so check with .T\n",
    "(W_v.weight.T @ W_v.weight).diag(), (W_v.weight @ W_v.weight.T).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d736ff-f9d1-4fbf-9f28-5e939fd16192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.8580, 3.8931, 4.2426, 3.8728, 4.1576, 4.0706, 3.9007, 3.9701, 3.8986,\n",
       "         3.7712, 3.9201, 4.0676, 3.6894, 3.9858, 3.9228, 4.1466, 3.7117, 3.7501,\n",
       "         4.0615, 3.8578, 4.2153, 3.8464, 3.6133, 3.8736, 3.8603, 3.9971, 3.8789,\n",
       "         4.1942, 4.1517, 3.8603, 3.9019, 3.4583, 3.5920, 3.8812, 3.9152, 3.9505,\n",
       "         4.1654, 4.3137, 4.2492, 3.8551, 4.0042, 4.0932, 4.0694, 3.7652, 4.2155,\n",
       "         4.0445, 4.1697, 3.9420, 3.9184, 4.0150, 4.0032, 4.0153, 3.7572, 3.9870,\n",
       "         3.9958, 3.5553, 3.8700, 4.0539, 4.0136, 3.9899, 3.5696, 4.1975, 4.1856,\n",
       "         3.6233, 3.7882, 3.9867, 3.8463, 3.9743, 3.9500, 4.0963, 4.0645, 4.3165,\n",
       "         4.0246, 4.0406, 4.0985, 3.7918, 3.7860, 3.9474, 4.4527, 4.1771, 4.3035,\n",
       "         4.0906, 4.1758, 3.8684, 3.7808, 4.1073, 4.1180, 3.7858, 4.5228, 3.8587,\n",
       "         4.0921, 3.8875, 4.1034, 4.1227, 3.9766, 3.7816, 3.7624, 4.1224, 4.0637,\n",
       "         3.8097, 3.7841, 3.7562, 3.9015, 3.9028, 3.8789, 3.9445, 4.1326, 4.0425,\n",
       "         3.9117, 4.0736, 4.1034, 3.9195, 3.6790, 3.6832, 3.9403, 4.2046, 4.2867,\n",
       "         3.8029, 4.3326, 3.9406, 4.3586, 3.8321, 3.8662, 3.9627, 3.6362, 3.8946,\n",
       "         4.3110, 4.2025, 3.9179, 3.9887, 4.2551, 4.1185, 3.6192, 4.0959, 4.0798,\n",
       "         3.8122, 4.1830, 3.9072, 4.0549, 3.9890, 3.8137, 3.9674, 3.9421, 3.8532,\n",
       "         3.9628, 4.1416, 4.1723, 4.0269, 3.9103, 4.1007, 4.1811, 3.7447, 4.3018,\n",
       "         4.0513, 4.0461, 4.3168, 3.8521, 3.9887, 4.0610, 3.9913, 3.9507, 3.7524,\n",
       "         4.0799, 3.9911, 3.9994, 4.1548, 3.7765, 3.9386, 3.9662, 3.7918, 4.1551,\n",
       "         4.1070, 3.6230, 4.0155, 3.8656, 4.0341, 4.1109, 4.2561, 3.7817, 4.0578,\n",
       "         4.4126, 4.1365, 3.9967, 3.6830, 3.8130, 4.2167, 3.8906, 3.9030, 3.6793,\n",
       "         3.9841, 3.8933, 4.1911, 3.9666, 3.8204, 4.2689, 4.0798, 4.1792, 3.9689,\n",
       "         4.4231, 4.0951, 3.9532, 4.0741, 3.9205, 4.1644, 4.0164, 4.2553, 4.0793,\n",
       "         4.0036, 3.9205, 3.6537, 4.1407, 3.8704, 4.0175, 4.0376, 3.7202, 3.7801,\n",
       "         3.9352, 3.9099, 3.8945, 4.0324, 3.8838, 4.0592, 4.2540, 4.0684, 4.3634,\n",
       "         3.8900, 4.0945, 4.1393, 3.8029, 4.2277, 3.6841, 3.6721, 3.9005, 3.7969,\n",
       "         3.9614, 3.9488, 3.7462, 3.6507, 4.0060, 4.2327, 4.2333, 4.0013, 3.9330,\n",
       "         4.0408, 4.2169, 4.0662, 4.0159, 4.1366, 3.8472, 3.7240, 3.7382, 4.0129,\n",
       "         4.0523, 3.9994, 4.1034, 3.9002]),\n",
       " tensor([1.1121, 0.9330, 0.9606,  ..., 0.9127, 0.8764, 0.9951]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_v_single_head = torch.normal(0, 1/math.sqrt(d_k), size=(d,d_k))\n",
    "\n",
    "(W_v_single_head.T @ W_v_single_head).diag(), (W_v_single_head @ W_v_single_head.T).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545596f-fbb2-4e0b-b3f1-1e4f37ebc903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd69212-c26e-4404-95a6-7442d6d814ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff823bd-ce40-49fd-b67b-5c12dc739614",
   "metadata": {},
   "source": [
    "### Check memorization on single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a33880-0afe-4de6-a594-682be1581cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeMoLayer(\n",
       "  (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "  (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "  (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,h,l = 1024, 4, 3\n",
    "\n",
    "layer = MeMoLayer(d, h)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fdbe2b1-b421-4f7e-8f61-18a82c9b0eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 12, 1024)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, current_length, d = input_embeddings.shape\n",
    "batch_size, current_length, d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7664712d-7ce1-4d64-b1f4-54abb019ffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 12, 1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8c91851-c546-48bf-90c6-bb4c7ed5484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4096) torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "current_length = int(input_embeddings.shape[1]/ h)\n",
    "\n",
    "input_sequence = input_embeddings.reshape((batch_size, current_length, h, d))\n",
    "\n",
    "current_output_symbols = output_symbols[:, [(x+1)*h-1 for x in range(0,current_length)]]\n",
    "j = 2 \n",
    "print(sum(sum(input_sequence[0][j] == input_embeddings[0][4*j:4*(j+1)])), input_sequence[0][j].shape)\n",
    "\n",
    "(batch_size, blocks,h,d) = input_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99480537-8c54-4dae-85dd-838ce5bfabb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3, 4, 1024]), torch.Size([52, 3, 1024]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence.shape, current_output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a2ba406-c0a0-4008-9aa0-836372382cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeMoLayer(\n",
       "  (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "  (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "  (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = MeMoLayer(d, h)\n",
    "display(layer)\n",
    "## update the input sequence for the next layer\n",
    "_, seq_encoding_for_the_last_layer = layer.memorize(input_sequence, current_output_symbols, is_last=False)\n",
    "layer.directly_memorize(seq_encoding_for_the_last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c46ccae-66bb-4e72-abaa-b7abdfd0839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3, 4, 1024]), torch.Size([3, 4, 1024]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence.shape, input_sequence[3].shape # batch (52 elements of chunks 4*4*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8467cfe3-4e46-45ea-b3fc-bac785a84ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 1024])\n"
     ]
    }
   ],
   "source": [
    "_, seq_encoding_for_the_last_layer = layer.retrieve(input_sequence)\n",
    "\n",
    "print(seq_encoding_for_the_last_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a3cd023-37a9-426a-9df2-b7d25a459ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = layer.directly_retrieve(seq_encoding_for_the_last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32a917d5-4a51-4163-9bac-21f960de5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([48505, 20110,  1108, 48019,    80, 19216,  9718,  1113,  4927,    66,\n",
      "        19216,  1448,  2122, 41530,   187,  4172,   246,   659, 10986, 30975,\n",
      "           80, 12931,   352, 14134,  2721,   258,  8830,    87,   826,    15,\n",
      "        17532,   729, 26798, 41070,  6575,   299,   266,  3737, 20889,   287,\n",
      "          512,   354,   250,   247,    70,   275, 16128,  2680,    74, 13679,\n",
      "           15,   209]) tensor([0.8087, 0.9320, 0.9849, 1.0984, 1.2244, 1.3176, 1.0163, 1.0889, 0.9557,\n",
      "        1.8996, 1.2476, 0.9599, 0.9788, 1.0013, 0.9819, 0.9918, 0.9456, 0.9295,\n",
      "        0.9705, 1.0572, 1.1345, 0.9096, 0.9802, 0.9546, 1.4618, 0.9903, 1.0166,\n",
      "        1.0116, 1.1751, 1.0554, 0.8882, 1.1188, 1.0335, 0.9970, 1.0631, 0.9999,\n",
      "        0.9998, 0.9425, 0.9801, 0.9525, 0.9181, 0.9766, 1.1350, 1.0746, 1.0994,\n",
      "        0.8784, 1.0130, 1.0656, 1.0741, 1.1679, 0.9479, 1.0618],\n",
      "       grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "retreived_output_symbol_vector, m = embedding.decode(logits)\n",
    "print(retreived_output_symbol_vector, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1acd6b66-770e-42d1-b01a-cce2d9bd2523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([48505, 20110,  1108, 48019,    80, 19216,  9718,  1113,  4927,    66,\n",
       "        19216,  1448,  2122, 41530,   187,  4172,   246,   659, 10986, 30975,\n",
       "           80, 12931,   352, 14134,  2721,   258,  8830,    87,   826,    15,\n",
       "        17532,   729, 26798, 41070,  6575,   299,   266,  3737, 20889,   287,\n",
       "          512,   354,   250,   247,    70,   275, 16128,  2680,    74, 13679,\n",
       "           15,   209])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52) over torch.Size([52])\n"
     ]
    }
   ],
   "source": [
    "o = embedding.decode(current_output_symbols[:, -1])[0]\n",
    "display(o)\n",
    "\n",
    "print(sum(o == retreived_output_symbol_vector), 'over', retreived_output_symbol_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fe7568a-9319-4beb-b56c-89c8fd366f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "370fac45-4707-43d5-88f8-df78ee1c931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 1024]) torch.Size([3, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "print(input_sequence[3].shape, current_output_symbols[3].shape)\n",
    "print(input_sequence[3][0].shape, current_output_symbols[3][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d28e7b-29c7-4368-9111-360fd75ab9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01cd5f78-d3a8-4be2-952a-2db537b10110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/156\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0 \n",
    "\n",
    "for batch_index in range(len(memo_input[0]['input_ids'])):\n",
    "    #print(\"input ids\", memo_input[0]['input_ids'][batch_index])\n",
    "    #print()\n",
    "    \n",
    "    for i in range(len(current_output_symbols[batch_index])):\n",
    "        #display(embedding.decode(input_sequence[batch_index][i]), embedding.decode(current_output_symbols[batch_index][i]))\n",
    "        true = embedding.decode(current_output_symbols[batch_index][i])[0].item()\n",
    "        \n",
    "        _, seq_encoding_for_the_last_layer  = layer.retrieve(input_sequence[batch_index][i].unsqueeze(0).unsqueeze(0))\n",
    "        \n",
    "        retreived_output_symbol_vector, m = embedding.decode(layer.directly_retrieve(seq_encoding_for_the_last_layer))\n",
    "        pred = retreived_output_symbol_vector.item()\n",
    "\n",
    "        total += 1\n",
    "        correct += pred == true\n",
    "\n",
    "print(f\"{correct}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592772e2-6b27-4071-bd5f-a441876381fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b22599b-d21d-43bc-b728-dbadd0e4263d",
   "metadata": {},
   "source": [
    "### Check with batch size of 1 and output probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30176822-1cf4-4b5e-8fbc-7e4375fdecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2520,  310,  247, 1071,  323,  247, 1077, 2159, 2159, 3425,  273, 1249]]) tensor([[  310,   247,  1071,   323,   247,  1077,  2159,  2159,  3425,   273,\n",
      "          1249, 21761]])\n",
      "(tensor([[  323,  2159, 21761]]), tensor([[1.0219, 0.9891, 1.0037]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4, 1024]), torch.Size([1, 3, 1024]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.get_text_batch_encoding(['this is a test for a very short short sequence of 12 tokens'])[0]\n",
    "input_ids, labels = memo_input['input_ids'], memo_input['labels']\n",
    "print(input_ids, labels)\n",
    "\n",
    "input_embeddings = embedding.encode(input_ids)\n",
    "#print(input_embeddings.shape)\n",
    "\n",
    "output_embeddings = embedding.encode(labels)\n",
    "#print(output_embeddings.shape)\n",
    "\n",
    "\n",
    "current_length = max_length\n",
    "\n",
    "current_length = int(current_length/h)\n",
    "input_sequence = input_embeddings.reshape((1, current_length, h, d))\n",
    "\n",
    "output_symbols = output_embeddings[:, [(x+1)*h-1 for x in range(0,current_length)]] ## the output symbol is always the same tokem?\n",
    "print(embedding.decode(output_symbols))\n",
    "input_sequence.shape, output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4968bbc-e2df-44ca-93ac-9610a1ab071c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2520,  310,  247, 1071],\n",
       "          [ 323,  247, 1077, 2159],\n",
       "          [2159, 3425,  273, 1249]]]),\n",
       " tensor([[  323,  2159, 21761]]),\n",
       " tensor([[2520,  310,  247, 1071,  323,  247, 1077, 2159, 2159, 3425,  273, 1249]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.decode(input_sequence)[0], embedding.decode(output_symbols)[0], input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cfaf35a-6ff8-4979-9727-cedcb78d8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccc5bed4-5902-4f96-bab2-f1eefe5207c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_symbols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "813fecba-8575-4e2b-8271-e94af40c2600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeMoLayer(\n",
       "  (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "  (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "  (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "tensor([323]) tensor([1.0180], grad_fn=<MaxBackward0>)\n",
      "torch.Size([1, 1024])\n",
      "tensor([2159]) tensor([1.0677], grad_fn=<MaxBackward0>)\n",
      "torch.Size([1, 1024])\n",
      "tensor([21761]) tensor([1.1110], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = MeMoLayer(d, h)\n",
    "display(layer)\n",
    "\n",
    "## update the input sequence for the next layer\n",
    "_, seq_encoding_for_the_last_layer = layer.memorize(input_sequence, output_symbols, is_last=False)\n",
    "layer.directly_memorize(seq_encoding_for_the_last_layer)\n",
    "\n",
    "for i in range(0,3):\n",
    "    _, seq_encoding_for_the_last_layer = layer.retrieve(input_sequence[0][i].unsqueeze(0).unsqueeze(0))\n",
    "    print(seq_encoding_for_the_last_layer.shape)\n",
    "                                                        \n",
    "    retreived_output_symbol_vector, m = embedding.decode(layer.directly_retrieve(seq_encoding_for_the_last_layer))\n",
    "    print(retreived_output_symbol_vector, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c89cd-cb85-4742-a214-c1fe1812e0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c3e709-edf1-4366-9766-ed6403ecedb9",
   "metadata": {},
   "source": [
    "## Test the entire MeMo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a73e8f4-a4d6-4dcf-9698-3c9853976b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoPyTorch.modelling_memo import MeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9113cb02-449d-4ff0-b0d4-0cf121004e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeMoPyTorch.modelling_memo_tokenizer import MeMoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a3e1627-b177-455c-9a81-7ab248fb1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testo_di_prova.txt\") as my_first_text_f:\n",
    "    my_first_text = my_first_text_f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c58b7e91-677e-44f4-a738-93b3d3bb9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'MeMoTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token and pad token id = <|endoftext|>, 0\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "print(max_length, h)\n",
    "tokenizer = MeMoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", \n",
    "                                          padding_side='left', truncation_side='left', \n",
    "                                          max_length=max_length, head_number=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cd21d5d-2e4e-4b8d-aebc-b8f78421a038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo_input = tokenizer.memo_heads_encode(my_first_text[0:10])\n",
    "memo_input[0]['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bacfc15c-56ea-49e3-9628-cec62cc9019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0' #'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e7021d0-26d9-448b-ba86-23c38c87499c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 4, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,h,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c2b3fb1-8a1c-4678-8831-da82aeece4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MeMo(\n",
       "  (encoder): MeMoEmbedding(50254, 1024, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x MeMoLayer(\n",
       "      (W_v_single_head): ProjectionTokens(in_features=1024, out_features=256)\n",
       "      (Prj): ProjectionSequence((trasposed wrt saved one) in_features=4096, out_features=1024)\n",
       "      (CMM): CorrelationMatrixMemory(in_features=1024, out_features=1024)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MeMo(inner_dim=d, \n",
    "             num_of_heads=h, \n",
    "             num_of_layers=3, \n",
    "             chunk_length=max_length, \n",
    "             num_embeddings=tokenizer.vocab_size, \n",
    "             padding_idx=tokenizer.pad_token_id, \n",
    "             device=device) #MeMoModel\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da7dce-28c9-4dcf-a6ba-3f7ea6a0ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b6ab2cc-10fc-4634-927d-8b2391baa8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def check_memorization(self, model, tokenizer, text, # device='cpu',\n",
    "                           starting_point=None):\n",
    "        if starting_point == None:\n",
    "            basic_block = model.h ** model.l\n",
    "        else:\n",
    "            basic_block = starting_point\n",
    "        \n",
    "        \n",
    "        input_ = tokenizer(my_first_text, padding='longest', truncation='do_not_truncate', max_length=None)\n",
    "        input_ = tokenizer.pad(input_, pad_to_multiple_of=basic_block)\n",
    "        input_ids = input_['input_ids']\n",
    "                \n",
    "        count = 0\n",
    "        correct = 0\n",
    "        max_length = tokenizer.max_length\n",
    "        (batch_size, number_of_tokens) = input_ids.shape\n",
    "\n",
    "        #print(f\"(batch_size, number_of_tokens) = {(batch_size, number_of_tokens)}\")\n",
    "        \n",
    "        for i in tqdm.tqdm(range(basic_block,  number_of_tokens - 1)):\n",
    "            text_tokens = input_ids[:, i - basic_block:i]\n",
    "            \n",
    "            (batch_size, number_of_tokens) = text_tokens.shape\n",
    "            \n",
    "            text_tokens = torch.concat((torch.zeros((batch_size, max_length-1-number_of_tokens), \n",
    "                                                    dtype=torch.int), \n",
    "                                        text_tokens), axis=1\n",
    "                                      )\n",
    "            \n",
    "            #print(i - basic_block, i)\n",
    "            out, max_value = model.retrieve(text_tokens)\n",
    "            #print(out, input_ids[:, i])\n",
    "            #print(out[0].item())\n",
    "            \n",
    "            count += batch_size\n",
    "            correct += torch.sum(out.to('cpu') == input_ids[:, i])\n",
    "        \n",
    "                           \n",
    "        return correct / count\n",
    "\n",
    "    def check_pretokenized(self, model, tokenizer, input_ids,# device='cpu',\n",
    "                           starting_point=None):\n",
    "        if starting_point == None:\n",
    "            basic_block = model.h ** model.l\n",
    "        else:\n",
    "            basic_block = starting_point\n",
    "                \n",
    "        count = 0\n",
    "        correct = 0\n",
    "        max_length = tokenizer.max_length\n",
    "        (batch_size, number_of_tokens) = input_ids.shape\n",
    "\n",
    "        #print(f\"(batch_size, number_of_tokens) = {(batch_size, number_of_tokens)}\")\n",
    "        \n",
    "        for i in tqdm.tqdm(range(basic_block,  number_of_tokens - 1)):\n",
    "            text_tokens = input_ids[:, i - basic_block:i]\n",
    "            \n",
    "            (batch_size, number_of_tokens) = text_tokens.shape\n",
    "            \n",
    "            text_tokens = torch.concat((torch.zeros((batch_size, max_length-1-number_of_tokens), \n",
    "                                                    dtype=torch.int), \n",
    "                                        text_tokens), axis=1\n",
    "                                      )\n",
    "            \n",
    "            #print(i - basic_block, i)\n",
    "            out, max_value = model.retrieve(text_tokens)\n",
    "            #print(out, input_ids[:, i])\n",
    "            #print(out[0].item())\n",
    "            \n",
    "            count += batch_size\n",
    "            correct += torch.sum(out.to('cpu') == input_ids[:, i])\n",
    "        \n",
    "                           \n",
    "        return correct / count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8a2bd82-a24f-4274-a2bf-26170f8abfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [00:00<00:00, 751.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization: %f  tensor(0.9310)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MeMo(inner_dim=d, \n",
    "             num_of_heads=h, \n",
    "             num_of_layers=l, \n",
    "             chunk_length=max_length, \n",
    "             num_embeddings=tokenizer.vocab_size, \n",
    "             padding_idx=tokenizer.pad_token_id, \n",
    "             device=device)\n",
    "\n",
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text]*8)\n",
    "model.memorize_text(memo_input)\n",
    "\n",
    "e = Evaluation()\n",
    "out = e.check_pretokenized(model, tokenizer, memo_input[0]['input_ids'])\n",
    "print(\"Degree of memorization: %f \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c72f7371-61f8-4f80-b332-ec7d8703d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeMo embedding initilialization\n",
      "CMM pre learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [00:00<00:00, 770.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization: %f  tensor(0.8574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MeMo(inner_dim=d, \n",
    "             num_of_heads=h, \n",
    "             num_of_layers=l, \n",
    "             chunk_length=max_length, \n",
    "             num_embeddings=tokenizer.vocab_size, \n",
    "             padding_idx=tokenizer.pad_token_id, \n",
    "             device=device)\n",
    "print(\"CMM pre learning\")\n",
    "display(model.layers[0].CMM.weight)\n",
    "\n",
    "\n",
    "\n",
    "memo_input = tokenizer.get_text_batch_encoding([my_first_text]*8)\n",
    "(bs, ml) = memo_input[0]['input_ids'].shape\n",
    "\n",
    "for b in range(bs):\n",
    "    memo_single_input = {h: {k: memo_input[h][k][b].unsqueeze(0) for k in memo_input[h]} for h in memo_input}\n",
    "    #print(memo_single_input[0]['input_ids'].shape)\n",
    "\n",
    "    model.memorize_text(memo_single_input)\n",
    "\n",
    "e = Evaluation()\n",
    "out = e.check_pretokenized(model, tokenizer, memo_input[0]['input_ids'])\n",
    "print(\"Degree of memorization: %f \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67c1f7ed-02cd-4f13-aa01-e8821b11f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9837,  0.0131,  0.0149,  ...,  0.0116, -0.0091,  0.0254],\n",
       "        [ 0.0131,  1.0073,  0.0030,  ..., -0.0221, -0.0078, -0.0083],\n",
       "        [ 0.0149,  0.0030,  1.0084,  ...,  0.0109,  0.0149, -0.0034],\n",
       "        ...,\n",
       "        [ 0.0116, -0.0221,  0.0109,  ...,  0.9977, -0.0294, -0.0096],\n",
       "        [-0.0091, -0.0078,  0.0149,  ..., -0.0294,  1.0146,  0.0030],\n",
       "        [ 0.0254, -0.0083, -0.0034,  ..., -0.0096,  0.0030,  1.0199]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prj = model.layers[0].Prj.weight.detach().cpu()\n",
    "CMM = model.layers[0].CMM.weight.detach().cpu()\n",
    "\n",
    "Prj.T @ Prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41935a13-0b0f-4b93-b129-0a1e88241fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.8936e-02, -1.9148e-02,  9.6987e-03,  ...,  3.7883e-02,\n",
       "         -1.9608e-02, -1.3408e-02],\n",
       "        [-2.8734e-02,  2.4769e-02,  6.7619e-03,  ...,  5.3196e-02,\n",
       "         -6.7219e-03, -2.7895e-02],\n",
       "        [-4.6326e-02, -5.1699e-02,  1.4755e-02,  ..., -5.4009e-02,\n",
       "          8.8325e-03,  3.6818e-02],\n",
       "        ...,\n",
       "        [ 1.3820e-02,  2.2205e-02,  2.1027e-02,  ...,  2.3433e-02,\n",
       "          3.4943e-02, -1.5351e-02],\n",
       "        [ 1.2819e-02,  1.3980e-02,  2.4138e-02,  ...,  1.4555e-02,\n",
       "          9.8114e-05,  1.5104e-02],\n",
       "        [ 1.2497e-02,  2.0969e-02,  1.2242e-02,  ...,  1.8295e-02,\n",
       "         -1.6273e-02, -1.4039e-02]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d237416-54ea-4b21-8d19-84599ff9e34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ad99bb2-af43-4e42-a447-6871f4ee47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [00:00<00:00, 763.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization: %f  tensor(0.8574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = e.check_pretokenized(model, tokenizer, memo_input[0]['input_ids'])\n",
    "print(\"Degree of memorization: %f \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "346e794d-b1c9-438e-b66f-14fd17c1ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forget_text(memo_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c4d5e5c-a117-4378-970a-d42b76a0ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [00:00<00:00, 742.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of memorization: %f  tensor(0.0909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = e.check_pretokenized(model, tokenizer, memo_input[0]['input_ids'])\n",
    "print(\"Degree of memorization: %f \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9e371-c48a-479a-8cec-fc87c9aaacdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
