{
 "cells": [
  {
   "cell_type": "code",
   "id": "80e10ce2-165f-4340-bd78-7fb2dfe6f624",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "def memorization_experiement(mem_rate = 0.0001, num_of_heads=4,embed_size=2048,embed_out_size=2048,vocab_size=2000,batch_size=80,num_of_iterations=1000, different_tokens=True,equal_tokens=0):\n",
    "    #Initialization of the embedding matrices\n",
    "    # Embedding for sequence elements\n",
    "    embed_seq_size = embed_size // num_of_heads\n",
    "    \n",
    "    seq_symbol_embed = torch.normal(0, 1/math.sqrt(embed_seq_size), size=(vocab_size,embed_seq_size))\n",
    "    \n",
    "    # Embedding for output elements # USING embed_seq_size instead of embed_size\n",
    "    out_symbol_embed = torch.normal(0, 1/math.sqrt(embed_size), size=(vocab_size,embed_out_size))\n",
    "\n",
    "    # Initialization of the key-value matrix\n",
    "    W0 = torch.zeros(size=(embed_size,embed_out_size), requires_grad=False)\n",
    "\n",
    "    np.random.seed(22)\n",
    "    results = []\n",
    "    stored_pairs = 0\n",
    "    avg_a = 1.0\n",
    "    step = 0\n",
    "    #for epoch in range(num_of_iterations):\n",
    "    while avg_a > 0.9:\n",
    "        samples = batch_size\n",
    "        if different_tokens:\n",
    "            input_seq = np.random.randint(0, vocab_size, size=(samples, num_of_heads))\n",
    "        else:\n",
    "            ### Sequences with only one different token \n",
    "            #equal_tokens = 2\n",
    "            input_seq = np.zeros((samples, num_of_heads))\n",
    "            input_sequence = np.random.randint(0, vocab_size, size=(1,num_of_heads))\n",
    "            for i in range(samples):\n",
    "                input_seq[i] = input_sequence\n",
    "            token_to_change = np.random.randint(0, vocab_size, size=(samples,num_of_heads-equal_tokens))\n",
    "            input_seq[:,0:num_of_heads-equal_tokens] = token_to_change\n",
    "            print(f\"IN {input_seq}\")\n",
    "        \n",
    "        output_symb = np.random.randint(0,vocab_size, size=(samples))\n",
    "\n",
    "        # embed_size x samples\n",
    "        keys = torch.transpose(seq_symbol_embed[:][input_seq.reshape(1, num_of_heads * samples)].reshape(samples, embed_size), 0, 1)\n",
    "        # samples x embed_size\n",
    "        values = out_symbol_embed[output_symb]\n",
    "\n",
    "        W0 = W0 + mem_rate*torch.matmul(keys,values)\n",
    "\n",
    "        if W0[0][0] > 10000.0:\n",
    "            W0 = normalize(W0)\n",
    "            print(\"normalized\")\n",
    "\n",
    "        #if epoch == num_of_iterations - 1 :\n",
    "        #    W0 = W0 - torch.matmul(keys_0, values_0)\n",
    "        #    print(\"Removing first batch\")\n",
    "\n",
    "\n",
    "\n",
    "        # samples x vocab_size\n",
    "        out = torch.argmax(torch.matmul(torch.matmul(torch.transpose(keys,0,1),W0),torch.transpose(out_symbol_embed,0,1)),dim=1)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        a = accuracy_score(output_symb,out)\n",
    "        stored_pairs += samples\n",
    "        if stored_pairs%(10*samples) == 0: print(f\".{stored_pairs}\", end=\"\")\n",
    "        if step == 0:\n",
    "            output_symb_0 = output_symb\n",
    "            keys_0 = keys\n",
    "            values_0 = values\n",
    "            a_0 = \"N/A\"\n",
    "            avg_a = a\n",
    "        else:\n",
    "            out = torch.argmax(\n",
    "                torch.matmul(torch.matmul(torch.transpose(keys_0, 0, 1), W0), torch.transpose(out_symbol_embed, 0, 1)),\n",
    "                dim=1)\n",
    "            out_0 = out.cpu().detach().numpy()\n",
    "            a_0 = accuracy_score(output_symb_0, out_0)\n",
    "            avg_a = (a + a_0)/2\n",
    "\n",
    "        #print(f\"{stored_pairs} \\t {a_0} \\t {a}\", end=\" - \")\n",
    "        step += 1\n",
    "    #print(\"procedure ended\")\n",
    "    return {\"stored_pairs\":stored_pairs, \"num_of_heads\":num_of_heads,\"embed_size\":embed_size,\"embed_OUT_size\":embed_out_size,\"vocab_size\":vocab_size,\n",
    "            \"batch_size\":batch_size}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93de1e87-80e9-4761-8cc8-89cb68b8c7b9",
   "metadata": {},
   "source": [
    "results = DataFrame()\n",
    "c_vocab_size = 100000\n",
    "\n",
    "for c_in_h_embed_size in [512//32,1024//32,2048//32,4096//32,8192//32]:\n",
    "    for c_out_size in [512,1024,2048,4096,8192]:\n",
    "        for c_head in [2,4,8,16,32]:\n",
    "            c_embed_size = c_in_h_embed_size*c_head\n",
    "            print(c_embed_size)\n",
    "            out = memorization_experiement(mem_rate = 1.0, num_of_iterations=100,vocab_size=c_vocab_size,\\\n",
    "                                            embed_size=c_embed_size, embed_out_size=c_out_size, num_of_heads=c_head, batch_size=1000)\n",
    "            results = pd.concat([results,DataFrame([out])])\n",
    "            print(out)\n",
    "results.to_excel(\"MemorizationResults.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32bb5f57-deb6-493b-8a05-f66d6d314016",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"MemorizationResults.xlsx\")\n",
    "size = 100000\n",
    "df[\"num_of_params\"] = df[\"embed_size\"]*df[\"embed_OUT_size\"]\n",
    "df[\"stored_pairs\"] = df[\"stored_pairs\"]\n",
    "#print(df)\n",
    "#for c_out_size in [512,1024,2048,4096,8192]:\n",
    "#for h in [2,4,8,16,32]: \n",
    "plt.plot(\"num_of_params\",\"stored_pairs\",data=df.loc[(df[\"vocab_size\"]==size)][[\"stored_pairs\",\n",
    "\"num_of_params\"]], label=f\"${c_out_size}$\",marker=\"*\",linewidth=0)\n",
    "\n",
    "x = df[\"num_of_params\"]\n",
    "y = df[\"stored_pairs\"]\n",
    "\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x,p(x),linestyle=\"solid\",linewidth=0.5 )\n",
    "\n",
    "plt.ylabel(\"Sequences\")\n",
    "plt.xlabel(\"Parameters\")\n",
    "#plt.title(\"vocabolary size=100,000\")\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"CMM_storing_capacity_vs_NoParameters.png\",dpi=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12a11eb1-a5ac-4cf1-87e8-c979f9292d04",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"MemorizationResults.xlsx\")\n",
    "size = 100000\n",
    "#df[\"num_of_params\"] = df[\"embed_size\"]*df[\"embed_OUT_size\"]\n",
    "df[\"stored_pairs\"] = df[\"stored_pairs\"]\n",
    "#print(df)\n",
    "for c_out_size in [512,1024,2048,4096,8192]:\n",
    "#for h in [2,4,8,16,32]: #&(df[\"embed_OUT_size\"]==c_out_size)&(df[\"embed_size\"]==c_out_size)\n",
    "    plt.plot(\"num_of_heads\",\"stored_pairs\",data=df.loc[(df[\"vocab_size\"]==size)&(df[\"embed_OUT_size\"]==c_out_size)&(df[\"embed_OUT_size\"]==df[\"embed_size\"])][[\"stored_pairs\",\n",
    "\"num_of_heads\"]],label = f\"$d$ = {c_out_size}\", marker=\"o\",linewidth=1)\n",
    "\n",
    "#x = df[\"num_of_params\"]\n",
    "#y = df[\"stored_pairs\"]\n",
    "\n",
    "#z = np.polyfit(x, y, 1)\n",
    "#p = np.poly1d(z)\n",
    "#plt.plot(x,p(x),linestyle=\"solid\",linewidth=0.5 )\n",
    "\n",
    "plt.ylabel(\"Sequences\")\n",
    "plt.xlabel(\"Parameters\")\n",
    "#plt.title(\"vocabolary size=100,000\")\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"CMM_storing_capacity_vs_dim.png\",dpi=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6ca53a6-235c-4873-8a69-0a2c02111d4e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
